{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                         Assignment 1 - Task 2 Text Pre-Processing \n",
    "\n",
    "### Student Name: Roopak TJ\n",
    "### Student ID: 29567467\n",
    "\n",
    "Date: 14th April, 2019\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "Libraries used: \n",
    "* Tabula (for reading and extracting data from pdf tables )\n",
    "* nltk - natural language toolkit (tokenizer, stemmer, collocations and probabilities)\n",
    "* matplotlib - For plots\n",
    "\n",
    "External files used:\n",
    "* Input file : 29567467.pdf | stopwords_en.txt\n",
    "* Output file : 29567467_vocab.txt | 29567467_countVec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "The process basically involves processing and formatting of data which can be fed into differnt machine learning and data analysis processes.The current task mainly covers:\n",
    "* Extracting data from PDF tables\n",
    "* Loading the data into Dataframe\n",
    "* Tokenization\n",
    "* Case Normalization\n",
    "* Removing stop words\n",
    "* Stemming technique\n",
    "* Count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabula import read_pdf\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using read_pdf function from Tabula package to extract tables in pdf to dataframe.\n",
    "Parameters: \n",
    "* file name\n",
    "* encoding technique\n",
    "* spreadsheet indicator boolean\n",
    "\n",
    "Returns:\n",
    "* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabula' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e08b1b35c70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import pdf to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"29567467.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspreadsheet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tabula' is not defined"
     ]
    }
   ],
   "source": [
    "# Import pdf to dataframe\n",
    "df = tabula.read_pdf(\"29567467.pdf\",encoding='utf-8', spreadsheet=True, pages='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-633337079cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On checking the data, it is noticed that some of the values in the cell contains ['NA'] in both Synopsis and Outcomes as seen below. This needs to be removed before text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows have Synopsis and Outcomes as \"NA\"\n",
    "df[df[\"Title\"] == \"AZA3644\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function which will consider such cases and replace ['NA'] with empty string.\n",
    "\n",
    "Parameters : String\n",
    "\n",
    "Return : Same string after removing null cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNull(s):\n",
    "    s = str(s)\n",
    "    if s == \"['NA']\":\n",
    "        s = \"\"\n",
    "    return s\n",
    "\n",
    "df[\"Synopsis\"] = df[\"Synopsis\"].apply(removeNull)\n",
    "df[\"Outcomes\"] = df[\"Outcomes\"].apply(removeNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the ['NA'] values has been replaced by empty string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows have Synopsis and Outcomes as \"NA\"\n",
    "df[df[\"Title\"] == \"AZA3644\"]\n",
    "df[df[\"Title\"] == \"LAW4227\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining 2 columns(Synopsis and Outcomes) into a single column Combination for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to combine synopsis and outcome\n",
    "df[\"Combination\"] = df.Synopsis + df.Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping the actual columns which contains redundant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop synopsis and outcome\n",
    "df.drop(columns = ['Synopsis', 'Outcomes'], inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function which will be called in the dataframe column which will perform the case Normalization and make every word except the Noun lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserveNoun(s):\n",
    "    s = str(s)\n",
    "    First = True\n",
    "    new = \"\"\n",
    "    flag = False\n",
    "    for each in s:\n",
    "        if flag and each != \" \":\n",
    "            each = each.lower()\n",
    "            flag = False\n",
    "        if each == \".\" or each == \"?\" or each == \"!\":\n",
    "            flag = True\n",
    "        if First:\n",
    "            each = each.lower()\n",
    "            First = False\n",
    "        new = new + each\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column to store the value returned from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new\"] = np.NAN\n",
    "df[\"new\"] = df[\"Combination\"].apply(preserveNoun)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping Combination column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Combination'], inplace = True, axis=1)\n",
    "df = df.rename(columns={'new': 'Information'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function tokenise which will generate unigrams from the string\n",
    "\n",
    "Parameters : String (Column value in Information)\n",
    "\n",
    "Returns : List of unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(s):\n",
    "    s = str(s)\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    unigram = tokenizer.tokenize(s)\n",
    "    return unigram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(tokenise)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams() : Creating some of the multigrams using MWETokenizer. Created a unique set of tokens. Appended the tuples of bigrams and used MWETokenizer to replace the existing values in tokens list to words separated by '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(s):\n",
    "\n",
    "    unique_set = list(set(s))\n",
    "    unique_set.append((\"transhistorical\", \"case\", \"studies\"))\n",
    "    unique_set.append((\"Post-Traumatic\", \"Stress\", \"Disorder\"))\n",
    "    unique_set.append((\"professional\", \"competency\"))\n",
    "    unique_set.append((\"occupational\", \"health\"))\n",
    "    \n",
    "    mwe_tokenizer = MWETokenizer(unique_set)\n",
    "    uni_and_bigrams = mwe_tokenizer.tokenize(s)\n",
    "    return uni_and_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(bigrams)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_three_or_less() : Function to remove the the words which have length less than 3. Such words are usually a part of stop words.\n",
    "\n",
    "Parameters: String (Values in the column Information)\n",
    "    \n",
    "Returns: Same String with the words of length less than 3 removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove tokens less than 3\n",
    "\n",
    "def remove_three_or_less(s):\n",
    "    new_list = []\n",
    "    for each in s:\n",
    "        if len(each) < 3:\n",
    "            continue\n",
    "        else:\n",
    "            new_list.append(each)\n",
    "            \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(remove_three_or_less)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided with a txt file containing stop words. Creating a function to read the data from the stopwords from the .txt file and removing the words from Information column which has words present in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    \n",
    "    stop_words = open(\"stopwords_en.txt\", 'r')\n",
    "    stop_words = stop_words.read()\n",
    "    stop_words = stop_words.split(\"\\n\")\n",
    "    stopped_list = [w for w in s if w not in stop_words]\n",
    "    return stopped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(remove_stop_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function which will get the words fromt the Information column and stem it to its base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(mwe_text):\n",
    "    \n",
    "    tagged_sent = nltk.tag.pos_tag(mwe_text)\n",
    "    lemmatizer = PorterStemmer()\n",
    "    \n",
    "    final_tokens = [lemmatizer.stem(w[0]) for w in tagged_sent ]\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(stem)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, we were handling tokens for each units. Now, we will iterate through the entire data frame to extract data from all units together into a global token list . Also using \"set\" to get unique tokens from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get complete tokens in a list\n",
    "count = df.shape[0]\n",
    "tokens = list()\n",
    "for index, row in df.head(n=count).iterrows():\n",
    "    for each in row[1]:\n",
    "        tokens.append(each)\n",
    "tokens_set = list(set(tokens))\n",
    "print(\"The number of global tokens: \" + str(len(tokens)))\n",
    "print(\"The number of unique tokens: \" + str(len(tokens_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first 200 meaningful bigrams, nltk.collocations.BigramAssocMeasures is used and this will be added to the existing unique set of tokens for updating the tokens already present in the tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 200 meaningful bigrams\n",
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "locater = nltk.collocations.BigramCollocationFinder.from_words(tokens_set)\n",
    "bestbigrams = locater.nbest(bigram.pmi, 200)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 200 to the existing set of global tokens\n",
    "token_set_bi = []\n",
    "\n",
    "for each in tokens_set:\n",
    "    token_set_bi.append(each)\n",
    "\n",
    "for each in bestbigrams:\n",
    "    token_set_bi.append(each)\n",
    "    \n",
    "print(\"Size of token set for MWE: \",len(token_set_bi))\n",
    "print(\"Size of initial unique token list: \", len(tokens_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noticed that the size of unique tokens has increased by 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the values in the existing tokens to bigrams using MWETokenizer class\n",
    "print(\"Before updating the bigrams, size of the tokens are : \",len(tokens_set))\n",
    "mwe_tokenizer = MWETokenizer(token_set_bi)\n",
    "tokens_set = mwe_tokenizer.tokenize(tokens_set)\n",
    "print(\"After updating the bigrams, size of the tokens are : \", len(tokens_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the bigrams, it can be noticed that the size of tokens have decreased below the count of original number of uniqur tokens. This is because we have combined tokens in the process. Alos updating the tokens in the dataframe with bigrams such that frequency can be calculated accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateColumn(s):\n",
    "    mwe_tokenizer = MWETokenizer(token_set_bi)\n",
    "    tokens = mwe_tokenizer.tokenize(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(updateColumn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using getTokenFrequency to get the frequency of tokens as a Dictionary and sorting the dictionary with respect to values to identify the most frequent and rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_token_frequency_dict = getTokenFrequency(tokens)\n",
    "global_token_frequency_dict = sorted(global_token_frequency_dict.items(), key=lambda x: x[1], reverse=False)\n",
    "print(global_token_frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(global_token_frequency_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to remove 5% of the global rare tokens, we will iterate through the dictionary till the count of 131(which was separately calculated to be 5% of the total number of unique tokens) and store the keys in a rare_tokens list. This rare_token list will be further used to remove from the actual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "rare_tokens = []\n",
    "for k,v in global_token_frequency_dict:\n",
    "    count += 1\n",
    "    if(count == 131):\n",
    "        break\n",
    "    rare_tokens.append(k)\n",
    "print(rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the rare tokens from the tokens present in dataframe column\n",
    "\n",
    "def remove_rare_words(s):\n",
    "    \n",
    "    rare_removed_list = [w for w in s if w not in rare_tokens]\n",
    "    return rare_removed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Information\"] = df[\"Information\"].apply(remove_rare_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FreqDist to get the frequency of tokens in the Information column as a separate TokenFrequency column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frequency column\n",
    "\n",
    "def getTokenFrequency(tokens):\n",
    "    fc = FreqDist(tokens)\n",
    "    fc = dict(fc)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TokenFrequency\"] = np.NAN\n",
    "df[\"TokenFrequency\"] = df[\"Information\"].apply(getTokenFrequency)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a token id dictionary in the format (token,id), we will iterate through all the unique tokens  and insert it to a dictionary assigning a value to it value which will be the count. Id starts with index 1 and not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for unique id\n",
    "\n",
    "token_id_dict = {}\n",
    "count = 0\n",
    "for each in tokens_set:\n",
    "    count += 1\n",
    "    token_id_dict[each] = count\n",
    "\n",
    "print(token_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the dictionary in the column Token frequency, using the key of the dictionary to find the value in the previously created global dictionary to get its token id and inserting it to a separate dictionary which is added to a separate column in the dataframe - IdFrequency. See the data in head display to understand better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_frequency(s):\n",
    "    tokenid_freq_dict = {}\n",
    "    for k, v in s.items():\n",
    "        if k in token_id_dict:\n",
    "            token_id = token_id_dict[k]\n",
    "            tokenid_freq_dict[token_id] = v\n",
    "    return tokenid_freq_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IdFrequency\"] = np.NAN\n",
    "df[\"IdFrequency\"] = df[\"TokenFrequency\"].apply(mapping_frequency)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the results to files - '29567467_vocab.txt' and '29567467_countVec.txt'.\n",
    "Global dictionary of token with its id is sorted first alphabetically and written to a file, each token in a separate line\n",
    "In order to write the count vector, we are iterating through the dataframe and writing the first(unitId) and fourth column(IdFrequency). Each unit is written to a separate line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to a file\n",
    "\n",
    "file_vocab = open('29567467_vocab.txt', 'w')\n",
    "\n",
    "# Sort dictionary with respect to key\n",
    "token_id_dict = dict(sorted(token_id_dict.items()))\n",
    "\n",
    "for k,v in token_id_dict.items():\n",
    "    file_vocab.write(str(k) + \":\" + str(v) + \"\\n\")\n",
    "file_vocab.close()\n",
    "\n",
    "file_frequency = open('29567467_countVec.txt', 'w')\n",
    "\n",
    "count = df.shape[0]\n",
    "for index, row in df.head(n=count).iterrows():\n",
    "    output = row[0]\n",
    "    for k,v in row[3].items():\n",
    "        output = output + \",\" + str(k) + \":\" + str(v)\n",
    "    output = output + \"\\n\"\n",
    "    file_frequency.write(output)\n",
    "file_frequency.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the most frequent words in the global tokens, we will plot the first 10 tokens along with their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_frequent = FreqDist(tokens)\n",
    "type(tokens_frequent.most_common(10))\n",
    "plot = {}\n",
    "for k,v in tokens_frequent.most_common(10):\n",
    "    plot[k] = v\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(len(plot)), list(plot.values()), align='center')\n",
    "plt.xticks(range(len(plot)), list(plot.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_frequent.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting frequency of frequency using a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying frequency distribution function on all the values of the tokens(k,v) which will have the frequency count\n",
    "fd = FreqDist(tokens_frequent.values())\n",
    "\n",
    "y = [0]*14\n",
    "for k, v in fd.items():\n",
    "     if k <= 10:\n",
    "        y[k-1] = v\n",
    "     elif k >10 and k <= 50:\n",
    "        y[10] =  y[10] + v\n",
    "     elif k >50 and k <= 100:\n",
    "        y[11] =  y[11] + v\n",
    "     elif k > 100 and k <= 500:\n",
    "        y[12] =  y[12] + v\n",
    "     else:\n",
    "        y[13] =  y[13] + v\n",
    "figure(figsize=(8,8))\n",
    "x = range(1, 15) \n",
    "ytks =list(map(str, range(1, 11))) \n",
    "ytks.append('10-50')\n",
    "ytks.append('51-100')\n",
    "ytks.append('101-400')\n",
    "barh(x,y)\n",
    "yticks(x, ytks)\n",
    "xlabel('Frequency of Frequency')\n",
    "ylabel('Word Frequency')\n",
    "grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Extracted data from PDF tables\n",
    "* Loaded the data into Dataframe\n",
    "* Completed word Tokenization\n",
    "* Did Case Normalization\n",
    "* Removed stop words\n",
    "* Stemming technique\n",
    "* Performed Count vectorization\n",
    "* Plotted charts of frequencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
